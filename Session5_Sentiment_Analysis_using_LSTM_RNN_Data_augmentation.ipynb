{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session5_Sentiment Analysis using LSTM RNN_Data_augmentation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDDyPN--FzCJ"
      },
      "source": [
        "!pip install google_trans_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bffAT2qdGLkG"
      },
      "source": [
        "import random\n",
        "import google_trans_new\n",
        "from google_trans_new import google_translator "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whNG3Op7GQT8"
      },
      "source": [
        "def random_insertion(sentence, n): \n",
        "    words = remove_stopwords(sentence) \n",
        "    for _ in range(n):\n",
        "        new_synonym = get_synonyms(random.choice(words))\n",
        "        sentence.insert(randrange(len(sentence)+1), new_synonym) \n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGRmuZq9GTkb"
      },
      "source": [
        "def random_swap(sentence, n=5): \n",
        "    length = range(len(sentence)) \n",
        "    for _ in range(n):\n",
        "        idx1, idx2 = random.sample(length, 2)\n",
        "        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1] \n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpuZY1QcGUTQ"
      },
      "source": [
        "def random_deletion(words, p=0.5): \n",
        "    if len(words) == 1: # return if single word\n",
        "        return words\n",
        "    remaining = list(filter(lambda x: random.uniform(0,1) > p,words)) \n",
        "    if len(remaining) == 0: # if not left, sample a random word\n",
        "        return [random.choice(words)] \n",
        "    else:\n",
        "        return remaining"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp5IzBGsPGHs"
      },
      "source": [
        "## Dataset Preview\n",
        "\n",
        "Your first step to deep learning in NLP. We will be mostly using PyTorch. Just like torchvision, PyTorch provides an official library, torchtext, for handling text-processing pipelines. \n",
        "\n",
        "We will be using previous session tweet dataset. Let's just preview the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1-Yz-5RRFYc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "e741be04-4d8b-4599-b544-3f8eb8bd5b47"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel('/content/tweets_-44724447.xlsx')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweets</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Obama has called the GOP budget social Darwini...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>In his teen years, Obama has been known to use...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>IPA Congratulates President Barack Obama for L...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RT @Professor_Why: #WhatsRomneyHiding - his co...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RT @wardollarshome: Obama has approved more ta...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              tweets  labels\n",
              "0  Obama has called the GOP budget social Darwini...       1\n",
              "1  In his teen years, Obama has been known to use...       0\n",
              "2  IPA Congratulates President Barack Obama for L...       0\n",
              "3  RT @Professor_Why: #WhatsRomneyHiding - his co...       0\n",
              "4  RT @wardollarshome: Obama has approved more ta...       1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7JdpCW-YbAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b0b7b6d-8c1c-4f4a-f8bf-4fdf131ea7e6"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1364, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqRsoF6xYdgl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3c557fd-4754-479c-c8a4-43e41ae730e7"
      },
      "source": [
        "df.labels.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    931\n",
              "1    352\n",
              "2     81\n",
              "Name: labels, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ6o_79ISSVb"
      },
      "source": [
        "## Defining Fields"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e63g08ijOrf7"
      },
      "source": [
        "Now we shall be defining LABEL as a LabelField, which is a subclass of Field that sets sequen tial to False (as it’s our numerical category class). TWEET is a standard Field object, where we have decided to use the spaCy tokenizer and convert all the text to lower‐ case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qk8IP4SK1Lrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9de0cb78-281f-4134-e985-f639beb4db8c"
      },
      "source": [
        "# Import Library\n",
        "import random\n",
        "# import torch, torchtext\n",
        "from torchtext import data \n",
        "import torch\n",
        "from torchtext.legacy import data\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Manual Seed\n",
        "SEED = 43\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fa5f8e74810>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6bKQax2Mf_U"
      },
      "source": [
        "Tweet = data.Field(sequential = True, tokenize = 'spacy', batch_first =True, include_lengths=True)\n",
        "Label = data.LabelField(tokenize ='spacy', is_target=True, batch_first =True, sequential =False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX-lYIe_O7Vy"
      },
      "source": [
        "Having defined those fields, we now need to produce a list that maps them onto the list of rows that are in the CSV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VawdWq36O6td"
      },
      "source": [
        "fields = [('tweets', Tweet),('labels',Label)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbtZ-Ph2P1xL"
      },
      "source": [
        "Armed with our declared fields, lets convert from pandas to list to torchtext. We could also use TabularDataset to apply that definition to the CSV directly but showing an alternative approach too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3OLcJ5B7rHz"
      },
      "source": [
        "example = [data.Example.fromlist([df.tweets[i],df.labels[i]], fields) for i in range(df.shape[0])] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT-flpH-P1cd"
      },
      "source": [
        "# Creating dataset\n",
        "#twitterDataset = data.TabularDataset(path=\"tweets.csv\", format=\"CSV\", fields=fields, skip_header=True)\n",
        "\n",
        "twitterDataset = data.Dataset(example, fields)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6ZnyCPaR08F"
      },
      "source": [
        "Finally, we can split into training, testing, and validation sets by using the split() method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPYXyuKhRpBk"
      },
      "source": [
        "(train, valid) = twitterDataset.split(split_ratio=[0.85, 0.15], random_state=random.seed(SEED))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmuwq40dUsgN"
      },
      "source": [
        "# Data Augmentatiob Process - convert the train sample generator to train data frame before applying the augmentation process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NYSvdcI0jFC",
        "outputId": "b765d3fc-804f-429b-9fc4-e4c0b4c5f35f"
      },
      "source": [
        "## Convert the train sample generator into train dataframe\n",
        "\n",
        "tweets=[]\n",
        "labels=[]\n",
        "for x in train.examples:\n",
        "  tweets.append(vars(x).get('tweets'))\n",
        "  labels.append(vars(x).get('labels'))\n",
        "\n",
        "df_train=pd.DataFrame({'Tweets':tweets,'Labels':labels})\n",
        "# print(tweets)\n",
        "print('\\n')\n",
        "# print(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "0    797\n",
            "1    293\n",
            "2     69\n",
            "Name: Labels, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0ZU7MS1Uc1e"
      },
      "source": [
        "# DATA Augmentation process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaIDXKuX3-Aa",
        "outputId": "b0826b02-faa4-45f8-a310-62912b6b17e6"
      },
      "source": [
        "## Perform data augmentation on train data frame\n",
        "# Random Delete\n",
        "# Random Swap\n",
        "# Back Translate\n",
        "\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from sklearn.utils import shuffle\n",
        "# Augmenting 20% of data\n",
        "\n",
        "samples_0=int(df_train['Labels'].value_counts()[0]*.2)\n",
        "samples_1=int(df_train['Labels'].value_counts()[1]*.2)\n",
        "samples_2=int(df_train['Labels'].value_counts()[2]*.2)\n",
        "\n",
        "\n",
        "    \n",
        "##selecting the class samples\n",
        "df_0=df_train[df_train.Labels==0].reset_index(drop=True)\n",
        "df_1=df_train[df_train.Labels==1].reset_index(drop=True)\n",
        "df_2=df_train[df_train.Labels==2].reset_index(drop=True)\n",
        "\n",
        "## Random Delete ####\n",
        "\n",
        "new_text=[]\n",
        "## data augmentation loop - class 0\n",
        "for i in tqdm(np.random.randint(0,len(df_0),samples_0)):\n",
        "  text = df_0.iloc[i]['Tweets']\n",
        "  augmented_text = random_deletion(text,0.1)\n",
        "  new_text.append(augmented_text)\n",
        "## dataframe\n",
        "rd_new_0=pd.DataFrame({'Tweets':new_text,'Labels':0})\n",
        "\n",
        "new_text=[]\n",
        "## data augmentation loop - class 1\n",
        "for i in tqdm(np.random.randint(0,len(df_1),samples_1)):\n",
        "  text = df_1.iloc[i]['Tweets']\n",
        "  augmented_text = random_deletion(text,0.1)\n",
        "  new_text.append(augmented_text)\n",
        "## dataframe\n",
        "rd_new_1=pd.DataFrame({'Tweets':new_text,'Labels':1})\n",
        "\n",
        "new_text=[]\n",
        "## data augmentation loop - class 2\n",
        "for i in tqdm(np.random.randint(0,len(df_2),samples_2)):\n",
        "  text = df_2.iloc[i]['Tweets']\n",
        "  augmented_text = random_deletion(text,0.1)\n",
        "  new_text.append(augmented_text)\n",
        "## dataframe\n",
        "rd_new_2=pd.DataFrame({'Tweets':new_text,'Labels':2})\n",
        "\n",
        "print('\\n',rd_new_0.shape,rd_new_1.shape,rd_new_2.shape)\n",
        "\n",
        "\n",
        "## Random Swap ####\n",
        "\n",
        "new_text=[]\n",
        "## data augmentation loop - class 0\n",
        "for i in tqdm(np.random.randint(0,len(df_0),samples_0)):\n",
        "  text = df_0.iloc[i]['Tweets']\n",
        "  augmented_text = random_swap(text)\n",
        "  new_text.append(augmented_text)\n",
        "## dataframe\n",
        "rs_new_0=pd.DataFrame({'Tweets':new_text,'Labels':0})\n",
        "\n",
        "new_text=[]\n",
        "## data augmentation loop - class 1\n",
        "for i in tqdm(np.random.randint(0,len(df_1),samples_1)):\n",
        "  text = df_1.iloc[i]['Tweets']\n",
        "  augmented_text = random_swap(text)\n",
        "  new_text.append(augmented_text)\n",
        "## dataframe\n",
        "rs_new_1=pd.DataFrame({'Tweets':new_text,'Labels':1})\n",
        "\n",
        "new_text=[]\n",
        "## data augmentation loop - class 2\n",
        "for i in tqdm(np.random.randint(0,len(df_2),samples_2)):\n",
        "  text = df_2.iloc[i]['Tweets']\n",
        "  augmented_text = random_swap(text)\n",
        "  new_text.append(augmented_text)\n",
        "## dataframe\n",
        "rs_new_2=pd.DataFrame({'Tweets':new_text,'Labels':2})\n",
        "\n",
        "print('\\n',rs_new_0.shape,rs_new_1.shape,rs_new_2.shape)\n",
        "\n",
        "\n",
        "## Back Translate ####\n",
        "\n",
        "translator=google_translator()\n",
        "available_langs = list(google_trans_new.LANGUAGES.keys()) \n",
        "\n",
        "new_text=[]\n",
        "## data augmentation loop - class 0\n",
        "for i in tqdm(np.random.randint(0,len(df_0),samples_0)):\n",
        "  trans_lang = random.choice(available_langs)\n",
        "  text = df_0.iloc[i]['Tweets']\n",
        "  translations=translator.translate(text, lang_tgt=trans_lang)\n",
        "  augmented_text=translator.translate(translations, lang_src=trans_lang, lang_tgt='en')\n",
        "  new_text.append(augmented_text)\n",
        "## dataframe\n",
        "bt_new_0=pd.DataFrame({'Tweets':new_text,'Labels':0})\n",
        "\n",
        "new_text=[]\n",
        "## data augmentation loop - class 1\n",
        "for i in tqdm(np.random.randint(0,len(df_1),samples_1)):\n",
        "  trans_lang = random.choice(available_langs)\n",
        "  text = df_1.iloc[i]['Tweets']\n",
        "  translations=translator.translate(text, lang_tgt=trans_lang)\n",
        "  augmented_text=translator.translate(translations, lang_src=trans_lang, lang_tgt='en')\n",
        "  new_text.append(augmented_text)\n",
        "## dataframe\n",
        "bt_new_1=pd.DataFrame({'Tweets':new_text,'Labels':1})\n",
        "\n",
        "new_text=[]\n",
        "## data augmentation loop - class 2\n",
        "for i in tqdm(np.random.randint(0,len(df_2),samples_2)):\n",
        "  trans_lang = random.choice(available_langs)\n",
        "  text = df_2.iloc[i]['Tweets']\n",
        "  translations=translator.translate(text, lang_tgt=trans_lang)\n",
        "  augmented_text=translator.translate(translations, lang_src=trans_lang, lang_tgt='en')\n",
        "  new_text.append(augmented_text)\n",
        "## dataframe\n",
        "bt_new_2=pd.DataFrame({'Tweets':new_text,'Labels':2})\n",
        "\n",
        "print('\\n',bt_new_0.shape,bt_new_1.shape,bt_new_2.shape)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 159/159 [00:00<00:00, 8063.53it/s]\n",
            "100%|██████████| 58/58 [00:00<00:00, 5798.35it/s]\n",
            "100%|██████████| 13/13 [00:00<00:00, 4597.47it/s]\n",
            "100%|██████████| 159/159 [00:00<00:00, 6879.31it/s]\n",
            "100%|██████████| 58/58 [00:00<00:00, 6465.81it/s]\n",
            "100%|██████████| 13/13 [00:00<00:00, 5922.87it/s]\n",
            "  0%|          | 0/159 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " (159, 2) (58, 2) (13, 2)\n",
            "\n",
            " (159, 2) (58, 2) (13, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 159/159 [01:58<00:00,  1.34it/s]\n",
            "100%|██████████| 58/58 [00:35<00:00,  1.62it/s]\n",
            "100%|██████████| 13/13 [00:10<00:00,  1.26it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " (159, 2) (58, 2) (13, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnZFHYdkGAgg",
        "outputId": "0ab1e60a-4ab7-4a44-9276-81a81dfa893f"
      },
      "source": [
        "df_train_new=shuffle(df_train.append([rd_new_0,rd_new_1,rd_new_2,rs_new_0,rs_new_1,rs_new_2,bt_new_0,bt_new_1,bt_new_2]).reset_index(drop=True)).reset_index(drop=True)\n",
        "print('\\n',df_train.head(),'\\n',df_train_new.head(),'\\n',df_train.shape,'\\n', df_train_new.shape )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "                                               Tweets  Labels\n",
            "0  [@billmaher, Mr, Maher, I, just, seen, the, HB...       0\n",
            "1  [Elections, #, of, @TBCDG, the, #, tcot, -, !,...       0\n",
            "2  [Huffington, blasting, GOP, praising, :, ., Ar...       1\n",
            "3  [portray, are, s, so, will, class, who, Romney...       2\n",
            "4  [believe, do, n't, I, there, ., is, in, good, ...       2 \n",
            "                                               Tweets  Labels\n",
            "0  [\"\" \"\" Mockingbird \",\" Obama \",\": \"\" Hosts \",\"...       0\n",
            "1  [RT, @DS_CT, :, It, 's, a, sorry, state, of, a...       0\n",
            "2  ['RT', '@ohgirlphrase', ':', 'US', 'has', '\",'...       0\n",
            "3  [RT, @ohgirlphrase, :, American, kid, \", You, ...       0\n",
            "4  [RT, @ohgirlphrase, :, American, kid, \", You, ...       0 \n",
            " (1159, 2) \n",
            " (1849, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpmKkoIO8vEO"
      },
      "source": [
        "# Create \"train_augmented\" generator from \"df_train_new\" augmented data frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykvsCGQMR6UD"
      },
      "source": [
        "example_augmented = [data.Example.fromlist([df_train_new.Tweets[i],df_train_new.Labels[i]], fields) for i in range(df_train_new.shape[0])] \n",
        "\n",
        "train_augmented = data.Dataset(example_augmented, fields)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQSbrQxhB7Cl",
        "outputId": "069110b9-891b-4c17-e8b6-fbc2b47b9749"
      },
      "source": [
        "train_augmented"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torchtext.legacy.data.dataset.Dataset at 0x7fa4f893df90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fih_AryUYJP7",
        "outputId": "f3872407-6dd0-44cf-d0fe-c89358cf6460"
      },
      "source": [
        "vars(train_augmented.examples[1])['labels']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kix8P2IKSBaV"
      },
      "source": [
        "An example from the augmented dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUpEOQruR9JL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63295be3-f788-44df-ef0b-c7f580511b2b"
      },
      "source": [
        "vars(train_augmented.examples[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'labels': 0,\n",
              " 'tweets': ['RT',\n",
              "  '@DS_CT',\n",
              "  ':',\n",
              "  'It',\n",
              "  \"'s\",\n",
              "  'a',\n",
              "  'sorry',\n",
              "  'state',\n",
              "  'of',\n",
              "  'affairs',\n",
              "  'when',\n",
              "  'the',\n",
              "  'US',\n",
              "  'President',\n",
              "  'menaces',\n",
              "  'other',\n",
              "  'branches',\n",
              "  'of',\n",
              "  'government',\n",
              "  '.',\n",
              "  'Obama',\n",
              "  'mocks',\n",
              "  'and',\n",
              "  'menaces',\n",
              "  'like',\n",
              "  'governing',\n",
              "  'is',\n",
              "  'sport',\n",
              "  '.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKdllP3FST4N"
      },
      "source": [
        "## Building Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuvWQ-SpSmSz"
      },
      "source": [
        "At this point we would have built a one-hot encoding of each word that is present in the dataset—a rather tedious process. Thankfully, torchtext will do this for us, and will also allow a max_size parameter to be passed in to limit the vocabu‐ lary to the most common words. This is normally done to prevent the construction of a huge, memory-hungry model. We don’t want our GPUs too overwhelmed, after all. \n",
        "\n",
        "Let’s limit the vocabulary to a maximum of 5000 words in our training set:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvyEeEjXTGhX"
      },
      "source": [
        "By default, torchtext will add two more special tokens, <unk> for unknown words and <pad>, a padding token that will be used to pad all our text to roughly the same size to help with efficient batching on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaFA5moQRNfO"
      },
      "source": [
        "# Tweet.build_vocab(train)\n",
        "# Label.build_vocab(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "800KwvAnRQ4M",
        "outputId": "55f13a1f-224e-46f2-d257-d7323a12213c"
      },
      "source": [
        "# print('Size of input vocab : ', len(Tweet.vocab))\n",
        "# print('Size of label vocab : ', len(Label.vocab))\n",
        "# print('Top 10 words appreared repeatedly :', list(Tweet.vocab.freqs.most_common(10)))\n",
        "# print('Labels : ', Label.vocab.stoi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  4651\n",
            "Size of label vocab :  3\n",
            "Top 10 words appreared repeatedly : [('Obama', 1069), (':', 783), ('#', 780), ('.', 761), (',', 598), ('\"', 550), ('the', 542), ('RT', 516), ('?', 419), ('to', 400)]\n",
            "Labels :  defaultdict(None, {0: 0, 1: 1, 2: 2})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx955u93SGeY"
      },
      "source": [
        "Tweet.build_vocab(train_augmented)\n",
        "Label.build_vocab(train_augmented)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwjD2-ebTeUX"
      },
      "source": [
        "**Lots of stopwords!!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA3tIESdcJdN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c08f1330-3d3d-4575-96bd-fac1eadb6d84"
      },
      "source": [
        "print('Size of input vocab : ', len(Tweet.vocab))\n",
        "print('Size of label vocab : ', len(Label.vocab))\n",
        "print('Top 10 words appreared repeatedly :', list(Tweet.vocab.freqs.most_common(10)))\n",
        "print('Labels : ', Label.vocab.stoi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  5431\n",
            "Size of label vocab :  3\n",
            "Top 10 words appreared repeatedly : [(\"'\", 6537), (',', 5388), ('\"', 3096), ('Obama', 1656), ('.', 1221), (':', 1206), ('#', 1183), (\"''\", 1130), ('the', 801), ('RT', 786)]\n",
            "Labels :  defaultdict(None, {0: 0, 1: 1, 2: 2})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLWW221gTpNs"
      },
      "source": [
        "Now we need to create a data loader to feed into our training loop. Torchtext provides the BucketIterator method that will produce what it calls a Batch, which is almost, but not quite, like the data loader we used on images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQqMhMoDUDmn"
      },
      "source": [
        "But at first declare the device we are using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfo2QhGJUK4l"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK2ORoqdTNsM"
      },
      "source": [
        "# Replace iterator -\"train\" with \"train_augmented\"\n",
        "train_iterator, valid_iterator = data.BucketIterator.splits((train_augmented, valid), batch_size = 32, \n",
        "                                                            sort_key = lambda x: len(x.tweets),\n",
        "                                                            sort_within_batch=True, device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg7gTFQO4fby"
      },
      "source": [
        "Save the vocabulary for later use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niE9Cc6-2bD_"
      },
      "source": [
        "import os, pickle\n",
        "with open('tokenizer.pkl', 'wb') as tokens: \n",
        "    pickle.dump(Tweet.vocab.stoi, tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AbsQwqkVyAy"
      },
      "source": [
        "## Defining Our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4PED4HJWH4t"
      },
      "source": [
        "We use the Embedding and LSTM modules in PyTorch to build a simple model for classifying tweets.\n",
        "\n",
        "In this model we create three layers. \n",
        "1. First, the words in our tweets are pushed into an Embedding layer, which we have established as a 300-dimensional vector embedding. \n",
        "2. That’s then fed into a 2 stacked-LSTMs with 100 hidden features (again, we’re compressing down from the 300-dimensional input like we did with images). We are using 2 LSTMs for using the dropout.\n",
        "3. Finally, the output of the LSTM (the final hidden state after processing the incoming tweet) is pushed through a standard fully connected layer with three outputs to correspond to our three possible classes (negative, positive, or neutral)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43pVRccMT0bT"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    \n",
        "    # Define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
        "        \n",
        "        super().__init__()          \n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           dropout=dropout,\n",
        "                           batch_first=True)\n",
        "        # try using nn.GRU or nn.RNN here and compare their performances\n",
        "        # try bidirectional and compare their performances\n",
        "        \n",
        "        # Dense layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        # text = [batch size, sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded = [batch size, sent_len, emb dim]\n",
        "      \n",
        "        # packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "    \n",
        "        # Hidden = [batch size, hid dim * num directions]\n",
        "        dense_outputs = self.fc(hidden)   \n",
        "        \n",
        "        # Final activation function softmax\n",
        "        output = F.softmax(dense_outputs[0], dim=1)\n",
        "            \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwBoGE_X_Fl8"
      },
      "source": [
        "# Define hyperparameters\n",
        "size_of_vocab = len(Tweet.vocab)\n",
        "embedding_dim = 300\n",
        "num_hidden_nodes = 100\n",
        "num_output_nodes = 3\n",
        "num_layers = 2\n",
        "dropout = 0.2\n",
        "\n",
        "# Instantiate the model\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout = dropout)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-pOMqzJ3eTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6890d9e4-3ee4-4dfe-c751-1f00aa7880f2"
      },
      "source": [
        "print(model)\n",
        "\n",
        "#No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifier(\n",
            "  (embedding): Embedding(4651, 300)\n",
            "  (encoder): LSTM(300, 100, num_layers=2, batch_first=True, dropout=0.2)\n",
            "  (fc): Linear(in_features=100, out_features=3, bias=True)\n",
            ")\n",
            "The model has 1,637,203 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXajorf5Xz7t"
      },
      "source": [
        "## Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrE9RpMtZ1Vs"
      },
      "source": [
        "First define the optimizer and loss functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u86JWdlXvu5"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# define optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    _, predictions = torch.max(preds, 1)\n",
        "    \n",
        "    correct = (predictions == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "    \n",
        "# push to cuda if available\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VCJtNb3Zt8w"
      },
      "source": [
        "The main thing to be aware of in this new training loop is that we have to reference `batch.tweets` and `batch.labels` to get the particular fields we’re interested in; they don’t fall out quite as nicely from the enumerator as they do in torchvision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WjEPLKsAiS_"
      },
      "source": [
        "**Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDWNnGK3Y5oJ"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    # initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # set the model in training phase\n",
        "    model.train()  \n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        # resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        # retrieve text and no. of words\n",
        "        tweet, tweet_lengths = batch.tweets   \n",
        "        \n",
        "        # convert to 1D tensor\n",
        "        predictions = model(tweet, tweet_lengths).squeeze()  \n",
        "        \n",
        "        # compute the loss\n",
        "        loss = criterion(predictions, batch.labels)        \n",
        "        \n",
        "        # compute the binary accuracy\n",
        "        acc = binary_accuracy(predictions, batch.labels)   \n",
        "        \n",
        "        # backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        # update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        # loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()    \n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZcHhkkvAsCt"
      },
      "source": [
        "**Evaluation Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHEe-zSVAriL"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    # initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    # deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    # deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            # retrieve text and no. of words\n",
        "            tweet, tweet_lengths = batch.tweets\n",
        "            \n",
        "            # convert to 1d tensor\n",
        "            predictions = model(tweet, tweet_lengths).squeeze()\n",
        "            \n",
        "            # compute loss and accuracy\n",
        "            loss = criterion(predictions, batch.labels)\n",
        "            acc = binary_accuracy(predictions, batch.labels)\n",
        "            \n",
        "            # keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6LJFW7HaJoV"
      },
      "source": [
        "**Let's Train and Evaluate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq330XlnaEU9"
      },
      "source": [
        "# N_EPOCHS = 10\n",
        "# best_valid_loss = float('inf')\n",
        "\n",
        "# for epoch in range(N_EPOCHS):\n",
        "     \n",
        "#     # train the model\n",
        "#     train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "#     # evaluate the model\n",
        "#     valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "#     # save the best model\n",
        "#     if valid_loss < best_valid_loss:\n",
        "#         best_valid_loss = valid_loss\n",
        "#         torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "#     print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "#     print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up8E5LFLTp2v"
      },
      "source": [
        "# Without Data Augmentation\n",
        "\n",
        "\n",
        "  Train Loss: 1.065 | Train Acc: 49.98%\n",
        "\t Val. Loss: 1.014 |  Val. Acc: 58.48% \n",
        "\n",
        "\tTrain Loss: 0.993 | Train Acc: 64.06%\n",
        "\t Val. Loss: 0.949 |  Val. Acc: 64.73% \n",
        "\n",
        "\tTrain Loss: 0.927 | Train Acc: 69.12%\n",
        "\t Val. Loss: 0.901 |  Val. Acc: 67.86% \n",
        "\n",
        "\tTrain Loss: 0.869 | Train Acc: 69.88%\n",
        "\t Val. Loss: 0.857 |  Val. Acc: 73.21% \n",
        "\n",
        "\tTrain Loss: 0.824 | Train Acc: 74.87%\n",
        "\t Val. Loss: 0.833 |  Val. Acc: 74.55% \n",
        "\n",
        "\tTrain Loss: 0.796 | Train Acc: 77.74%\n",
        "\t Val. Loss: 0.820 |  Val. Acc: 75.89% \n",
        "\n",
        "\tTrain Loss: 0.776 | Train Acc: 78.67%\n",
        "\t Val. Loss: 0.807 |  Val. Acc: 75.89% \n",
        "\n",
        "\tTrain Loss: 0.760 | Train Acc: 80.24%\n",
        "\t Val. Loss: 0.802 |  Val. Acc: 76.79% \n",
        "\n",
        "\tTrain Loss: 0.746 | Train Acc: 81.67%\n",
        "\t Val. Loss: 0.797 |  Val. Acc: 77.23% \n",
        "\n",
        "\tTrain Loss: 0.731 | Train Acc: 83.28%\n",
        "\t Val. Loss: 0.791 |  Val. Acc: 77.68% "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCrM4hl-UTig"
      },
      "source": [
        "# With Data Augmentation (20% of train samples)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE54k6BcR_lq",
        "outputId": "687ead13-bbbf-480c-ec4d-12c8005aa362"
      },
      "source": [
        "print('Results after data augmentation','\\n')\n",
        "N_EPOCHS = 10\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    # train the model\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "    # evaluate the model\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    # save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after data augmentation \n",
            "\n",
            "\tTrain Loss: 1.071 | Train Acc: 47.06%\n",
            "\t Val. Loss: 1.000 |  Val. Acc: 66.96% \n",
            "\n",
            "\tTrain Loss: 0.968 | Train Acc: 67.47%\n",
            "\t Val. Loss: 0.891 |  Val. Acc: 70.54% \n",
            "\n",
            "\tTrain Loss: 0.867 | Train Acc: 70.44%\n",
            "\t Val. Loss: 0.833 |  Val. Acc: 71.88% \n",
            "\n",
            "\tTrain Loss: 0.823 | Train Acc: 73.94%\n",
            "\t Val. Loss: 0.814 |  Val. Acc: 73.21% \n",
            "\n",
            "\tTrain Loss: 0.797 | Train Acc: 76.09%\n",
            "\t Val. Loss: 0.796 |  Val. Acc: 76.34% \n",
            "\n",
            "\tTrain Loss: 0.781 | Train Acc: 77.98%\n",
            "\t Val. Loss: 0.784 |  Val. Acc: 77.68% \n",
            "\n",
            "\tTrain Loss: 0.765 | Train Acc: 79.49%\n",
            "\t Val. Loss: 0.776 |  Val. Acc: 77.68% \n",
            "\n",
            "\tTrain Loss: 0.750 | Train Acc: 81.32%\n",
            "\t Val. Loss: 0.772 |  Val. Acc: 77.68% \n",
            "\n",
            "\tTrain Loss: 0.733 | Train Acc: 83.69%\n",
            "\t Val. Loss: 0.762 |  Val. Acc: 79.02% \n",
            "\n",
            "\tTrain Loss: 0.719 | Train Acc: 85.20%\n",
            "\t Val. Loss: 0.759 |  Val. Acc: 79.02% \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZgzB0ZkHVTI"
      },
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZZfnWo0abRx"
      },
      "source": [
        "#load weights and tokenizer\n",
        "\n",
        "path='./saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path));\n",
        "model.eval();\n",
        "tokenizer_file = open('./tokenizer.pkl', 'rb')\n",
        "tokenizer = pickle.load(tokenizer_file)\n",
        "\n",
        "#inference \n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def classify_tweet(tweet):\n",
        "    \n",
        "    categories = {0: \"Negative\", 1:\"Positive\", 2:\"Neutral\"}\n",
        "    \n",
        "    # tokenize the tweet \n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(tweet)] \n",
        "    # convert to integer sequence using predefined tokenizer dictionary\n",
        "    indexed = [tokenizer[t] for t in tokenized]        \n",
        "    # compute no. of words        \n",
        "    length = [len(indexed)]\n",
        "    # convert to tensor                                    \n",
        "    tensor = torch.LongTensor(indexed).to(device)   \n",
        "    # reshape in form of batch, no. of words           \n",
        "    tensor = tensor.unsqueeze(1).T  \n",
        "    # convert to tensor                          \n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # Get the model prediction                  \n",
        "    prediction = model(tensor, length_tensor)\n",
        "\n",
        "    _, pred = torch.max(prediction, 1) \n",
        "    \n",
        "    return categories[pred.item()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTkHLEipIlM9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ec165c39-9dc3-4880-ad7e-9f845eb78253"
      },
      "source": [
        "classify_tweet(\"A valid explanation for why Trump won't let women on the golf course.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Negative'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 221
        }
      ]
    }
  ]
}